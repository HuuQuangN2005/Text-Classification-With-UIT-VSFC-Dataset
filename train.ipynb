{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed1eed9",
   "metadata": {},
   "source": [
    "**0. Import Lib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2925ea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "from dataset import UITVSFCDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b7e4de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)                 \n",
    "    np.random.seed(seed)              \n",
    "    torch.manual_seed(seed)           \n",
    "    torch.cuda.manual_seed(seed)      \n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(1410)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f23e07",
   "metadata": {},
   "source": [
    "**1. Import Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9b2f315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'data/UIT-VSFC'\n",
    "VI_SYNONYM_PATH = 'data/synonym_vi.csv'\n",
    "VI_STOPWORD_PATH = 'data/vietnamese-stopwords.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5a283514",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = UITVSFCDataset(root_dir=DATASET_PATH).load_data()\n",
    "synonym_file = pd.read_csv(VI_SYNONYM_PATH)\n",
    "\n",
    "with open(VI_STOPWORD_PATH,'r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "vi_stopwords = [line.rstrip('\\n') for line in lines]\n",
    "\n",
    "synonym_dict = {}\n",
    "for index, row in synonym_file.iterrows():\n",
    "    word = row['word'].strip()\n",
    "    synonyms = [s.strip() for s in row['synonyms'].split(';')]\n",
    "    synonym_dict[word] = synonyms\n",
    " \n",
    "sentiment_labels = {\n",
    "    0: 'negative',\n",
    "    1: 'neutral',\n",
    "    2: 'positive'\n",
    "}\n",
    "\n",
    "topic_labels = {\n",
    "    0: 'lecturer',\n",
    "    1: 'training program',\n",
    "    2: 'facility',\n",
    "    3: 'other'\n",
    "}\n",
    "\n",
    "emoji_labels = {\n",
    "    ':))': 'colonsmilesmile',\n",
    "    ':)':  'colonsmile',\n",
    "\t':(':  'colonsad',\n",
    "\t'@@':  'colonsurprise',\n",
    "\t'<3':  'colonlove',\n",
    "\t':d':  'colonsmilesmile',\n",
    "\t':3':  'coloncontemn',\n",
    "\t':v':  'colonbigsmile',\n",
    "\t':_':  'coloncc',\n",
    "\t':p':  'colonsmallsmile',\n",
    "\t'>>':  'coloncolon',\n",
    "\t':\">': 'colonlovelove',\n",
    "\t'^^': 'colonhihi',\n",
    "\t':': 'doubledot',\n",
    "\t':(': 'colonsadcolon',\n",
    "\t':’(': 'colonsadcolon',\n",
    "\t':@': 'colondoublesurprise',\n",
    "\t'v.v': 'vdotv',\n",
    "\t'...': 'dotdotdot',\n",
    "\t'/': 'fraction',\n",
    "\t'c#': 'cshrap'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "daa34463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'đẹp': ['xinh', 'tuyệt đẹp', 'dễ thương', 'đẹp đẽ'],\n",
       " 'tốt': ['ổn', 'xuất sắc', 'tuyệt vời', 'chất lượng'],\n",
       " 'xấu': ['tệ', 'kém', 'dở', 'kém chất lượng'],\n",
       " 'nhanh': ['mau', 'lẹ', 'tốc độ', 'nhanh chóng'],\n",
       " 'chậm': ['từ tốn', 'ì ạch', 'không nhanh'],\n",
       " 'vui': ['hạnh phúc', 'phấn khởi', 'vui vẻ'],\n",
       " 'buồn': ['chán', 'rầu rĩ', 'buồn bã'],\n",
       " 'giận': ['tức', 'cáu', 'nổi nóng'],\n",
       " 'mệt': ['đuối', 'kiệt sức', 'mệt mỏi'],\n",
       " 'khó': ['phức tạp', 'gian nan', 'không dễ'],\n",
       " 'dễ': ['đơn giản', 'dễ dàng', 'thuận tiện'],\n",
       " 'to': ['lớn', 'bự', 'khổng lồ'],\n",
       " 'nhỏ': ['bé', 'tí hon', 'nhỏ nhắn'],\n",
       " 'sai': ['không đúng', 'nhầm', 'lệch'],\n",
       " 'đúng': ['chính xác', 'chuẩn', 'hợp lý'],\n",
       " 'mua': ['sắm', 'đặt mua', 'lấy'],\n",
       " 'bán': ['trao đổi', 'cung cấp', 'kinh doanh'],\n",
       " 'đắt': ['cao giá', 'mắc', 'giá cao'],\n",
       " 'rẻ': ['giá thấp', 'bình dân', 'phải chăng'],\n",
       " 'ngon': ['tuyệt ngon', 'thơm ngon', 'đậm vị'],\n",
       " 'dở': ['tệ', 'kém ngon', 'khó ăn'],\n",
       " 'được': ['ổn', 'hợp lý', 'được việc'],\n",
       " 'không': ['chẳng', 'không hề', 'không có'],\n",
       " 'cần': ['cần thiết', 'phải có', 'quan trọng'],\n",
       " 'thích': ['yêu thích', 'ưa thích', 'mê'],\n",
       " 'ghét': ['không thích', 'chán ghét', 'khó chịu'],\n",
       " 'cao': ['lớn', 'vượt trội', 'cao ráo'],\n",
       " 'thấp': ['nhỏ', 'ngắn', 'kém'],\n",
       " 'dài': ['lâu', 'kéo dài', 'dài dòng'],\n",
       " 'ngắn': ['cụt', 'gọn', 'vắn tắt'],\n",
       " 'mới': ['vừa', 'mới toanh', 'mới tinh'],\n",
       " 'cũ': ['xưa', 'cũ kỹ', 'lỗi thời'],\n",
       " 'hỏng': ['lỗi', 'không hoạt động', 'bị trục trặc'],\n",
       " 'bẩn': ['dơ', 'dơ bẩn', 'không sạch'],\n",
       " 'sạch': ['tinh tươm', 'sạch sẽ', 'gọn gàng'],\n",
       " 'đông': ['nhiều người', 'tấp nập', 'đông đúc'],\n",
       " 'vắng': ['thưa', 'ít người', 'vắng vẻ'],\n",
       " 'khó chịu': ['phiền', 'bực', 'khó ở'],\n",
       " 'dễ chịu': ['thoải mái', 'thư giãn', 'yên tâm'],\n",
       " 'quan trọng': ['thiết yếu', 'trọng yếu', 'cần thiết'],\n",
       " 'nhanh chóng': ['lập tức', 'ngay', 'gấp'],\n",
       " 'chậm rãi': ['từ từ', 'thong thả', 'nhẹ nhàng'],\n",
       " 'hài lòng': ['thỏa mãn', 'ưng', 'vừa ý'],\n",
       " 'không hài lòng': ['thất vọng', 'không vui', 'không ưng'],\n",
       " 'đơn giản': ['dễ', 'không phức tạp', 'nhẹ nhàng'],\n",
       " 'phức tạp': ['rắc rối', 'khó', 'nhiều bước']}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonym_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "221ee062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a lô', 'a ha', 'ai', 'ai ai', 'ai nấy']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vi_stopwords[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78806889",
   "metadata": {},
   "source": [
    "**2. Preprocess Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ef84af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDA():\n",
    "    def __init__(self, synonym_dict):\n",
    "        self.synonym_dict = synonym_dict\n",
    "    \n",
    "    def process(self,tokens:list,eda_probs = 0.3) -> list:\n",
    "        augmented_tokens = tokens\n",
    "        \n",
    "        if random.random() < eda_probs:\n",
    "            augmented_tokens = self.synonym_augmentation(tokens,prob=0.4)\n",
    "    \n",
    "        if random.random() < eda_probs:\n",
    "            augmented_tokens = self.random_insertion(augmented_tokens, n=1)        \n",
    "    \n",
    "        if random.random() < eda_probs:\n",
    "            augmented_tokens = self.random_swap(augmented_tokens, n=1)\n",
    "    \n",
    "        if random.random() < eda_probs:\n",
    "            augmented_tokens = self.random_deletion(augmented_tokens, word_threshold=3, p=0.1)\n",
    "    \n",
    "        return augmented_tokens\n",
    "    \n",
    "    def synonym_augmentation(self,tokens,prob=0.3):\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in self.synonym_dict and random.random() < prob:\n",
    "                synonyms = self.synonym_dict[token]\n",
    "                if synonyms:\n",
    "                    new_token = random.choice(synonyms)\n",
    "                    new_tokens.append(new_token)\n",
    "                else:\n",
    "                    new_tokens.append(token)\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        return new_tokens\n",
    "    \n",
    "    def random_insertion(self,tokens, n=2):\n",
    "        new_tokens = tokens.copy()\n",
    "    \n",
    "        for _ in range(n):\n",
    "            token = random.choice(new_tokens).lower()\n",
    "        \n",
    "            if token not in self.synonym_dict:\n",
    "                continue\n",
    "        \n",
    "            synonym = random.choice(synonym_dict[token])\n",
    "            insert_pos = random.randint(0, len(new_tokens))\n",
    "            new_tokens.insert(insert_pos, synonym)\n",
    "        \n",
    "        return new_tokens\n",
    "    \n",
    "    def random_swap(self,tokens, n= 1):\n",
    "        len_tokens = len(tokens)\n",
    "    \n",
    "        if len_tokens < 2:\n",
    "            return tokens\n",
    "    \n",
    "        new_tokens = tokens.copy()\n",
    "    \n",
    "        for _ in range(n):\n",
    "            i = random.randint(0, len_tokens - 1)\n",
    "            j = random.randint(0, len_tokens - 1)\n",
    "        \n",
    "            while i == j:\n",
    "                j = random.randint(0, len_tokens - 1)\n",
    "        \n",
    "            new_tokens[i], new_tokens[j] = new_tokens[j], new_tokens[i]\n",
    "        \n",
    "        return new_tokens\n",
    "    \n",
    "    def random_deletion(self,tokens, word_threshold = 3, p=0.2):\n",
    "        if len(tokens) < word_threshold:\n",
    "            return tokens\n",
    "    \n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            if random.random() > p:\n",
    "                new_tokens.append(token)\n",
    "    \n",
    "        if len(new_tokens) == 0:\n",
    "            return [random.choice(tokens)]\n",
    "    \n",
    "        return new_tokens\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b125e4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocess():\n",
    "    def __init__(self,stopwords,tokenizer, emoji_labels, transform = None):\n",
    "        self.stopwords = stopwords\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.emoji_labels = emoji_labels\n",
    "        self.__emoji_sorted = sorted(self.emoji_labels.keys(), key=len, reverse=True)\n",
    "        self.__emoji_regex = \"|\".join(re.escape(k) for k in self.__emoji_sorted)\n",
    "        self.__pattern = re.compile(self.__emoji_regex, flags=re.IGNORECASE)\n",
    "\n",
    "        self.vietnamese_pattern = r\"[^a-zA-Z0-9\\sàáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵđĐ]\"\n",
    "\n",
    "        self.transform = transform\n",
    "        \n",
    "    def process(self, text:str, apply_transform =True) -> list:\n",
    "        \n",
    "        text = text.strip()\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.lower()\n",
    "        \n",
    "        text = self.replace_emoji(text)\n",
    "\n",
    "        placeholders = re.findall(r\"<[^<>]+>\", text)\n",
    "        for i, ph in enumerate(placeholders):\n",
    "            text = text.replace(ph, f\"__PLACEHOLDER{i}__\")\n",
    "        \n",
    "        text = re.sub(self.vietnamese_pattern, \"\", text, flags=re.UNICODE)\n",
    "        \n",
    "        for i, ph in enumerate(placeholders):\n",
    "            text = text.replace(f\"__PLACEHOLDER{i}__\", ph)\n",
    "                \n",
    "        tokens = self.tokenize(text)\n",
    "        \n",
    "        if self.transform is not None and apply_transform == True:\n",
    "            tokens = self.transform.process(tokens)\n",
    "        \n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        tokens = self.remove_accents(tokens)\n",
    "       \n",
    "        #tokens = [t for t in tokens if not re.match(r'^\\s+$', t)]\n",
    "        return tokens\n",
    "        \n",
    "    def __repl(self,match:str):\n",
    "        key = match.group().lower()\n",
    "        return \" \" + self.emoji_labels[key] + \" \"\n",
    "    \n",
    "    def replace_emoji(self,text:str):\n",
    "        return self.__pattern.sub(self.__repl,text)\n",
    "        \n",
    "    def remove_accents(self,tokens:list) -> list:\n",
    "        new_tokens = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            nfkd = unicodedata.normalize('NFD', token)\n",
    "            token = ''.join([c for c in nfkd if not unicodedata.category(c) == 'Mn'])\n",
    "            new_tokens.append(token)    \n",
    "\n",
    "        return new_tokens\n",
    "    \n",
    "    def remove_stopwords(self,tokens:list) -> list:\n",
    "        new_tokens = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in self.stopwords:\n",
    "                continue\n",
    "            \n",
    "            new_tokens.append(token)\n",
    "            \n",
    "        return new_tokens\n",
    "    \n",
    "    def tokenize(self, text:str) -> list:\n",
    "        tokens = [token.text for token in self.tokenizer(text)]\n",
    "        \n",
    "        cleaned_token = []\n",
    "        for token in tokens:\n",
    "            if re.match(r'^\\s*$', token):\n",
    "                continue\n",
    "            \n",
    "            if token in self.emoji_labels.values():\n",
    "                cleaned_token.append(f\"<{token}>\")\n",
    "                continue\n",
    "        \n",
    "            if re.search(r'\\d', token):\n",
    "                if re.fullmatch(r'\\d+', token):\n",
    "                    cleaned_token.append(\"<NUM>\")\n",
    "                else:\n",
    "                    cleaned_token.append(\"<UNK>\")\n",
    "            else:\n",
    "                cleaned_token.append(token)\n",
    "                        \n",
    "        return cleaned_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4571298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = spacy.blank('vi')\n",
    "\n",
    "eda = EDA(synonym_dict=synonym_dict)\n",
    "preprocess = DataPreprocess(stopwords=vi_stopwords,\n",
    "                            tokenizer=tokenizer,\n",
    "                            emoji_labels=emoji_labels,\n",
    "                            transform=eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "606a0b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: do trình độ tiếng anh của lớp không cao chỉ một số ít có khả năng nghe , đọc , và hiểu được bài giảng của thấy nên hiệu quả của việc giảng dạy bằng tiếng anh là chưa cao .:)) @@ 143 42342 colonsmilesmile\n",
      "preprocess: \n",
      "['trinh đo', 'tieng', 'lop', 'mot so it', 'kha nang', 'đoc', 'on', 'giang', 'hieu qua', 'giang day', 'tieng', '<colonsmilesmile>', '<colonsurprise>', '<NUM>', '<NUM>', '<colonsmilesmile>']\n"
     ]
    }
   ],
   "source": [
    "example = dataset['test'][170]['sentence'] + \":)) @@ 143 42342 colonsmilesmile\"\n",
    "print(\"example: \" + example)\n",
    "print(\"preprocess: \")\n",
    "print(preprocess.process(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09ba7d7",
   "metadata": {},
   "source": [
    "**3. Build Vocab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "7b588d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVocab():\n",
    "    def __init__(self,dataset = None,synonym_dict:dict = None,emoji_labels:dict = None,preprocess = None):\n",
    "        self.preprocess = preprocess\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.synonym_dict = synonym_dict\n",
    "        self.emoji_labels = emoji_labels\n",
    "        \n",
    "        self.__vocab = set()\n",
    "        \n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "        \n",
    "        self.vocab_size = None\n",
    "        \n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        if self.preprocess is None or self.dataset is None or self.synonym_dict is None or self.emoji_labels is None:\n",
    "            return None\n",
    "        for key, syn_list in self.synonym_dict.items(): \n",
    "            key_norm = self.preprocess.remove_accents([key])[0]\n",
    "            self.__vocab.add(key_norm)\n",
    "        \n",
    "        for v in syn_list:\n",
    "            v_norm = self.preprocess.remove_accents([v])[0]\n",
    "            self.__vocab.add(v_norm)\n",
    "\n",
    "        for value in self.emoji_labels.values():\n",
    "            self.__vocab.add(value)\n",
    "            \n",
    "        for _, row in self.dataset['train'].items():\n",
    "            tokens = self.preprocess.process(row['sentence'],False)\n",
    "            self.__vocab.update(tokens)\n",
    "                \n",
    "        special_tokens = [\"<SOS>\",\"<EOS>\",\"<PAD>\", \"<UNK>\",\"<NUM>\"]\n",
    "        \n",
    "        final_vocab = special_tokens + sorted(self.__vocab)\n",
    "       \n",
    "        self.idx2word = final_vocab\n",
    "        \n",
    "        self.word2idx = {w: i for i, w in enumerate(final_vocab)}\n",
    "\n",
    "        self.vocab_size = len(final_vocab)\n",
    "        \n",
    "        print(\"Vocab size:\", self.vocab_size)\n",
    "        return self.word2idx\n",
    "    \n",
    "    def save_vocab(self, path=\"vocab.txt\"):\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for word in self.idx2word:\n",
    "                f.write(word + \"\\n\")\n",
    "        \n",
    "    def load_vocab(self, path=\"vocab.txt\"):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            vocab = [line.strip() for line in f.readlines()]\n",
    "\n",
    "        self.idx2word = vocab\n",
    "        self.word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "        self.vocab_size = len(vocab)\n",
    "        print(\"Loaded vocab size:\", self.vocab_size)\n",
    "\n",
    "    def mapping(self, tokens):\n",
    "        return [self.word2idx.get(t, self.word2idx[\"<UNK>\"]) for t in tokens]\n",
    "    \n",
    "    def reverse_mapping(self, indices: list) -> list:\n",
    "        return [self.idx2word[i] if i < len(self.idx2word) else \"<UNK>\" for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8c817789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 2598\n"
     ]
    }
   ],
   "source": [
    "vocab = MyVocab(\n",
    "    preprocess=preprocess,\n",
    "    emoji_labels=emoji_labels,\n",
    "    dataset=dataset,\n",
    "    synonym_dict=synonym_dict\n",
    ")\n",
    "\n",
    "word2idx = vocab.build_vocab()\n",
    "vocab.save_vocab(\"vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6ad77d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab size: 2598\n"
     ]
    }
   ],
   "source": [
    "test_vocab = MyVocab()\n",
    "test_vocab.load_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "cdd3b3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: thầy dạy hay và có tâm , mặc dù môn học lịch sử khá khô khan nhưng chúng em hiểu được rất nhiều bài học qua bài giảng của thầy .\n",
      "process: \n",
      "['thay', 'day', 'tam', 'mac du', 'mon hoc', 'lich su', 'kho khan', 'bai hoc', 'giang', 'thay']\n",
      "mapping: \n",
      "[1875, 490, 1792, 1142, 1221, 1062, 930, 62, 649, 1875]\n",
      "reverse_mapping: \n",
      "['thay', 'day', 'tam', 'mac du', 'mon hoc', 'lich su', 'kho khan', 'bai hoc', 'giang', 'thay']\n"
     ]
    }
   ],
   "source": [
    "example = dataset['dev'][32]['sentence']\n",
    "print(\"example: \" + example)\n",
    "print(\"process: \")\n",
    "test_process = preprocess.process(example,False)\n",
    "print(test_process)\n",
    "print('mapping: ' )\n",
    "test_mapping = test_vocab.mapping(test_process)\n",
    "print(test_mapping)\n",
    "print('reverse_mapping: ')\n",
    "print(test_vocab.reverse_mapping(test_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000e5387",
   "metadata": {},
   "source": [
    "**4. Build Dataset and DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ab6157c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VSFCDataset(Dataset):\n",
    "    def __init__(self, data, preprocess, vocab):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.preprocess = preprocess\n",
    "        self.vocab = vocab\n",
    "        self.sos_idx = vocab.word2idx[\"<SOS>\"]\n",
    "        self.eos_idx = vocab.word2idx[\"<EOS>\"]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        tokens = self.preprocess.process(sample['sentence'])\n",
    "        token_ids = self.vocab.mapping(tokens)\n",
    "        token_ids = [self.sos_idx] + token_ids + [self.eos_idx]\n",
    "        sentiment_label = int(sample['sentiment'])\n",
    "        topic_label = int(sample['topic'])\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "            'sentiment': torch.tensor(sentiment_label, dtype=torch.long),\n",
    "            'topic': torch.tensor(topic_label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch, pad_idx):\n",
    "    \n",
    "    input_ids = [b['input_ids'] for b in batch]\n",
    "    sentiment = torch.stack([b['sentiment'] for b in batch])\n",
    "    topic = torch.stack([b['topic'] for b in batch])\n",
    "    \n",
    "    max_len = max(seq.size(0) for seq in input_ids)\n",
    "    padded_inputs = []\n",
    "    for seq in input_ids:\n",
    "        pad_len = max_len - seq.size(0)\n",
    "        padded_seq = torch.cat([seq, torch.full((pad_len,), pad_idx, dtype=torch.long)])\n",
    "        padded_inputs.append(padded_seq)\n",
    "    \n",
    "    input_ids = torch.stack(padded_inputs) \n",
    "    \n",
    "    return {'input_ids': input_ids, 'sentiment': sentiment, 'topic': topic}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "8acb5b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = int(vocab.word2idx[\"<PAD>\"])\n",
    "\n",
    "train_dataset = VSFCDataset(dataset['train'], preprocess, vocab)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=200,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: collate_fn(x, pad_idx),\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "test_dataset = VSFCDataset(dataset['dev'], preprocess, vocab)\n",
    "val_dataset = VSFCDataset(dataset['test'], preprocess, vocab)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=200,\n",
    "    shuffle=False,          \n",
    "    collate_fn=lambda x: collate_fn(x, pad_idx)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=200,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: collate_fn(x, pad_idx)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0718a79f",
   "metadata": {},
   "source": [
    "**5. Build Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "aec59955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VSFCClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size,  sentiment_classes , topic_classes, pad_idx,embed_dim = 128, hidden_dim =256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim,\n",
    "            padding_idx=pad_idx\n",
    "            )\n",
    "        \n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size= embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True, \n",
    "            bidirectional=True\n",
    "            )\n",
    "        \n",
    "        self.attn = nn.Linear(\n",
    "            in_features=hidden_dim*2,\n",
    "            out_features=1\n",
    "            )\n",
    "        \n",
    "        self.sentiment_head = nn.Linear(\n",
    "            in_features=hidden_dim*2,\n",
    "            out_features=sentiment_classes\n",
    "        )\n",
    "        \n",
    "        self.topic_head = nn.Linear(\n",
    "            in_features = hidden_dim*2,\n",
    "            out_features = topic_classes\n",
    "        )\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        enc_out, _ = self.encoder(x)\n",
    "        \n",
    "        attn_weights = torch.softmax(self.attn(enc_out), dim= 1)\n",
    "        pooled = torch.sum(enc_out*attn_weights,dim = 1 )\n",
    "        \n",
    "        sentiment_logits = self.sentiment_head(pooled)\n",
    "        topic_logits = self.topic_head(pooled)\n",
    "        \n",
    "        return sentiment_logits, topic_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "4e1dbb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VSFCClassifier(\n",
    "    vocab_size=vocab.get_vocab_size(),\n",
    "    sentiment_classes=len(sentiment_labels),\n",
    "    topic_classes=len(topic_labels),\n",
    "    pad_idx=vocab.word2idx[\"<PAD>\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464af174",
   "metadata": {},
   "source": [
    "**6. Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "74e61cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 3060\n",
      "Current device index: 0\n",
      "Memory allocated: 21549568\n",
      "Memory cached: 1472200704\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Current device index:\", torch.cuda.current_device())\n",
    "    print(\"Memory allocated:\", torch.cuda.memory_allocated(0))\n",
    "    print(\"Memory cached:\", torch.cuda.memory_reserved(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e24a79bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path=\"saved_model/vsfc_model.pth\"):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to: {path}\")\n",
    "    \n",
    "def load_model(model_class, vocab_size, embed_dim, hidden_dim, \n",
    "               sentiment_classes, topic_classes, path, device,pad_idx):\n",
    "\n",
    "    model = model_class(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=embed_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        sentiment_classes=sentiment_classes,\n",
    "        topic_classes=topic_classes,\n",
    "        pad_idx = pad_idx\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7dcce3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train(model, train_loader, val_loader, device, num_epochs=5, lr=1e-3,save_plot_dir=\"plots\"):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    os.makedirs(save_plot_dir, exist_ok=True)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_sent_accs = []\n",
    "    val_topic_accs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            sentiment_labels = batch['sentiment'].to(device)\n",
    "            topic_labels = batch['topic'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            s_logits, t_logits = model(input_ids)\n",
    "            loss = criterion(s_logits, sentiment_labels) + criterion(t_logits, topic_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        correct_sent, total_sent = 0, 0\n",
    "        correct_topic, total_topic = 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                sentiment_labels = batch['sentiment'].to(device)\n",
    "                topic_labels = batch['topic'].to(device)\n",
    "                \n",
    "                s_logits, t_logits = model(input_ids)\n",
    "                s_pred = s_logits.argmax(dim=1)\n",
    "                t_pred = t_logits.argmax(dim=1)\n",
    "                \n",
    "                correct_sent += (s_pred == sentiment_labels).sum().item()\n",
    "                total_sent += sentiment_labels.size(0)\n",
    "                \n",
    "                correct_topic += (t_pred == topic_labels).sum().item()\n",
    "                total_topic += topic_labels.size(0)\n",
    "        \n",
    "        sent_acc = correct_sent / total_sent\n",
    "        topic_acc = correct_topic / total_topic\n",
    "        val_sent_accs.append(sent_acc)\n",
    "        val_topic_accs.append(topic_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss={avg_loss:.4f}, \"\n",
    "              f\"Val Sentiment Acc={sent_acc:.4f}, Val Topic Acc={topic_acc:.4f}\")\n",
    "    \n",
    "    save_model(model=model)\n",
    "\n",
    "    \n",
    "    epochs = range(1, num_epochs+1)\n",
    "    \n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(epochs, train_losses, marker='o', label='Train Loss')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_plot_dir, \"train_loss.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(epochs, val_sent_accs, marker='o', label='Val Sentiment Acc')\n",
    "    plt.plot(epochs, val_topic_accs, marker='s', label='Val Topic Acc')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Validation Accuracy\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_plot_dir, \"val_accuracy.png\"))\n",
    "    plt.close()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3df21458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss=1.4385, Val Sentiment Acc=0.7243, Val Topic Acc=0.7934\n",
      "Epoch 2/100, Loss=1.0176, Val Sentiment Acc=0.7612, Val Topic Acc=0.8225\n",
      "Epoch 3/100, Loss=0.9197, Val Sentiment Acc=0.7720, Val Topic Acc=0.8313\n",
      "Epoch 4/100, Loss=0.8661, Val Sentiment Acc=0.7713, Val Topic Acc=0.8354\n",
      "Epoch 5/100, Loss=0.8113, Val Sentiment Acc=0.7751, Val Topic Acc=0.8380\n",
      "Epoch 6/100, Loss=0.7740, Val Sentiment Acc=0.7780, Val Topic Acc=0.8440\n",
      "Epoch 7/100, Loss=0.7315, Val Sentiment Acc=0.7846, Val Topic Acc=0.8364\n",
      "Epoch 8/100, Loss=0.6877, Val Sentiment Acc=0.7836, Val Topic Acc=0.8399\n",
      "Epoch 9/100, Loss=0.6437, Val Sentiment Acc=0.7827, Val Topic Acc=0.8395\n",
      "Epoch 10/100, Loss=0.6155, Val Sentiment Acc=0.7900, Val Topic Acc=0.8326\n",
      "Epoch 11/100, Loss=0.5733, Val Sentiment Acc=0.7858, Val Topic Acc=0.8354\n",
      "Epoch 12/100, Loss=0.5582, Val Sentiment Acc=0.7858, Val Topic Acc=0.8339\n",
      "Epoch 13/100, Loss=0.5155, Val Sentiment Acc=0.7846, Val Topic Acc=0.8332\n",
      "Epoch 14/100, Loss=0.4857, Val Sentiment Acc=0.7846, Val Topic Acc=0.8335\n",
      "Epoch 15/100, Loss=0.4518, Val Sentiment Acc=0.7729, Val Topic Acc=0.8332\n",
      "Epoch 16/100, Loss=0.4478, Val Sentiment Acc=0.7748, Val Topic Acc=0.8291\n",
      "Epoch 17/100, Loss=0.4221, Val Sentiment Acc=0.7798, Val Topic Acc=0.8316\n",
      "Epoch 18/100, Loss=0.3948, Val Sentiment Acc=0.7824, Val Topic Acc=0.8244\n",
      "Epoch 19/100, Loss=0.3903, Val Sentiment Acc=0.7723, Val Topic Acc=0.8260\n",
      "Epoch 20/100, Loss=0.3668, Val Sentiment Acc=0.7805, Val Topic Acc=0.8244\n",
      "Epoch 21/100, Loss=0.3572, Val Sentiment Acc=0.7767, Val Topic Acc=0.8282\n",
      "Epoch 22/100, Loss=0.3495, Val Sentiment Acc=0.7767, Val Topic Acc=0.8269\n",
      "Epoch 23/100, Loss=0.3377, Val Sentiment Acc=0.7773, Val Topic Acc=0.8263\n",
      "Epoch 24/100, Loss=0.3343, Val Sentiment Acc=0.7618, Val Topic Acc=0.8256\n",
      "Epoch 25/100, Loss=0.3228, Val Sentiment Acc=0.7716, Val Topic Acc=0.8285\n",
      "Epoch 26/100, Loss=0.3067, Val Sentiment Acc=0.7789, Val Topic Acc=0.8288\n",
      "Epoch 27/100, Loss=0.3234, Val Sentiment Acc=0.7783, Val Topic Acc=0.8285\n",
      "Epoch 28/100, Loss=0.3117, Val Sentiment Acc=0.7795, Val Topic Acc=0.8260\n",
      "Epoch 29/100, Loss=0.2926, Val Sentiment Acc=0.7770, Val Topic Acc=0.8247\n",
      "Epoch 30/100, Loss=0.2968, Val Sentiment Acc=0.7726, Val Topic Acc=0.8288\n",
      "Epoch 31/100, Loss=0.2966, Val Sentiment Acc=0.7805, Val Topic Acc=0.8275\n",
      "Epoch 32/100, Loss=0.2901, Val Sentiment Acc=0.7720, Val Topic Acc=0.8294\n",
      "Epoch 33/100, Loss=0.2871, Val Sentiment Acc=0.7852, Val Topic Acc=0.8370\n",
      "Epoch 34/100, Loss=0.2809, Val Sentiment Acc=0.7682, Val Topic Acc=0.8260\n",
      "Epoch 35/100, Loss=0.2728, Val Sentiment Acc=0.7764, Val Topic Acc=0.8260\n",
      "Epoch 36/100, Loss=0.2707, Val Sentiment Acc=0.7729, Val Topic Acc=0.8294\n",
      "Epoch 37/100, Loss=0.2666, Val Sentiment Acc=0.7770, Val Topic Acc=0.8256\n",
      "Epoch 38/100, Loss=0.2690, Val Sentiment Acc=0.7650, Val Topic Acc=0.8275\n",
      "Epoch 39/100, Loss=0.2623, Val Sentiment Acc=0.7745, Val Topic Acc=0.8335\n",
      "Epoch 40/100, Loss=0.2692, Val Sentiment Acc=0.7802, Val Topic Acc=0.8307\n",
      "Epoch 41/100, Loss=0.2659, Val Sentiment Acc=0.7742, Val Topic Acc=0.8288\n",
      "Epoch 42/100, Loss=0.2651, Val Sentiment Acc=0.7783, Val Topic Acc=0.8241\n",
      "Epoch 43/100, Loss=0.2543, Val Sentiment Acc=0.7761, Val Topic Acc=0.8260\n",
      "Epoch 44/100, Loss=0.2536, Val Sentiment Acc=0.7745, Val Topic Acc=0.8279\n",
      "Epoch 45/100, Loss=0.2556, Val Sentiment Acc=0.7735, Val Topic Acc=0.8298\n",
      "Epoch 46/100, Loss=0.2559, Val Sentiment Acc=0.7732, Val Topic Acc=0.8279\n",
      "Epoch 47/100, Loss=0.2467, Val Sentiment Acc=0.7767, Val Topic Acc=0.8348\n",
      "Epoch 48/100, Loss=0.2461, Val Sentiment Acc=0.7710, Val Topic Acc=0.8298\n",
      "Epoch 49/100, Loss=0.2515, Val Sentiment Acc=0.7776, Val Topic Acc=0.8269\n",
      "Epoch 50/100, Loss=0.2380, Val Sentiment Acc=0.7817, Val Topic Acc=0.8294\n",
      "Epoch 51/100, Loss=0.2369, Val Sentiment Acc=0.7713, Val Topic Acc=0.8263\n",
      "Epoch 52/100, Loss=0.2382, Val Sentiment Acc=0.7685, Val Topic Acc=0.8320\n",
      "Epoch 53/100, Loss=0.2439, Val Sentiment Acc=0.7710, Val Topic Acc=0.8307\n",
      "Epoch 54/100, Loss=0.2419, Val Sentiment Acc=0.7704, Val Topic Acc=0.8298\n",
      "Epoch 55/100, Loss=0.2398, Val Sentiment Acc=0.7701, Val Topic Acc=0.8316\n",
      "Epoch 56/100, Loss=0.2366, Val Sentiment Acc=0.7802, Val Topic Acc=0.8256\n",
      "Epoch 57/100, Loss=0.2327, Val Sentiment Acc=0.7675, Val Topic Acc=0.8301\n",
      "Epoch 58/100, Loss=0.2394, Val Sentiment Acc=0.7641, Val Topic Acc=0.8266\n",
      "Epoch 59/100, Loss=0.2382, Val Sentiment Acc=0.7732, Val Topic Acc=0.8282\n",
      "Epoch 60/100, Loss=0.2399, Val Sentiment Acc=0.7685, Val Topic Acc=0.8285\n",
      "Epoch 61/100, Loss=0.2306, Val Sentiment Acc=0.7723, Val Topic Acc=0.8282\n",
      "Epoch 62/100, Loss=0.2368, Val Sentiment Acc=0.7606, Val Topic Acc=0.8282\n",
      "Epoch 63/100, Loss=0.2300, Val Sentiment Acc=0.7846, Val Topic Acc=0.8266\n",
      "Epoch 64/100, Loss=0.2387, Val Sentiment Acc=0.7821, Val Topic Acc=0.8244\n",
      "Epoch 65/100, Loss=0.2351, Val Sentiment Acc=0.7697, Val Topic Acc=0.8247\n",
      "Epoch 66/100, Loss=0.2291, Val Sentiment Acc=0.7723, Val Topic Acc=0.8222\n",
      "Epoch 67/100, Loss=0.2336, Val Sentiment Acc=0.7738, Val Topic Acc=0.8241\n",
      "Epoch 68/100, Loss=0.2298, Val Sentiment Acc=0.7713, Val Topic Acc=0.8294\n",
      "Epoch 69/100, Loss=0.2295, Val Sentiment Acc=0.7742, Val Topic Acc=0.8238\n",
      "Epoch 70/100, Loss=0.2315, Val Sentiment Acc=0.7710, Val Topic Acc=0.8269\n",
      "Epoch 71/100, Loss=0.2302, Val Sentiment Acc=0.7776, Val Topic Acc=0.8234\n",
      "Epoch 72/100, Loss=0.2286, Val Sentiment Acc=0.7723, Val Topic Acc=0.8253\n",
      "Epoch 73/100, Loss=0.2297, Val Sentiment Acc=0.7757, Val Topic Acc=0.8231\n",
      "Epoch 74/100, Loss=0.2262, Val Sentiment Acc=0.7726, Val Topic Acc=0.8256\n",
      "Epoch 75/100, Loss=0.2301, Val Sentiment Acc=0.7792, Val Topic Acc=0.8234\n",
      "Epoch 76/100, Loss=0.2238, Val Sentiment Acc=0.7726, Val Topic Acc=0.8238\n",
      "Epoch 77/100, Loss=0.2335, Val Sentiment Acc=0.7697, Val Topic Acc=0.8279\n",
      "Epoch 78/100, Loss=0.2276, Val Sentiment Acc=0.7707, Val Topic Acc=0.8222\n",
      "Epoch 79/100, Loss=0.2255, Val Sentiment Acc=0.7697, Val Topic Acc=0.8256\n",
      "Epoch 80/100, Loss=0.2273, Val Sentiment Acc=0.7789, Val Topic Acc=0.8222\n",
      "Epoch 81/100, Loss=0.2225, Val Sentiment Acc=0.7754, Val Topic Acc=0.8285\n",
      "Epoch 82/100, Loss=0.2193, Val Sentiment Acc=0.7789, Val Topic Acc=0.8275\n",
      "Epoch 83/100, Loss=0.2171, Val Sentiment Acc=0.7764, Val Topic Acc=0.8301\n",
      "Epoch 84/100, Loss=0.2190, Val Sentiment Acc=0.7732, Val Topic Acc=0.8253\n",
      "Epoch 85/100, Loss=0.2278, Val Sentiment Acc=0.7672, Val Topic Acc=0.8187\n",
      "Epoch 86/100, Loss=0.2201, Val Sentiment Acc=0.7751, Val Topic Acc=0.8279\n",
      "Epoch 87/100, Loss=0.2160, Val Sentiment Acc=0.7748, Val Topic Acc=0.8234\n",
      "Epoch 88/100, Loss=0.2175, Val Sentiment Acc=0.7764, Val Topic Acc=0.8307\n",
      "Epoch 89/100, Loss=0.2224, Val Sentiment Acc=0.7644, Val Topic Acc=0.8215\n",
      "Epoch 90/100, Loss=0.2177, Val Sentiment Acc=0.7697, Val Topic Acc=0.8256\n",
      "Epoch 91/100, Loss=0.2179, Val Sentiment Acc=0.7773, Val Topic Acc=0.8279\n",
      "Epoch 92/100, Loss=0.2227, Val Sentiment Acc=0.7757, Val Topic Acc=0.8228\n",
      "Epoch 93/100, Loss=0.2104, Val Sentiment Acc=0.7751, Val Topic Acc=0.8313\n",
      "Epoch 94/100, Loss=0.2167, Val Sentiment Acc=0.7751, Val Topic Acc=0.8231\n",
      "Epoch 95/100, Loss=0.2143, Val Sentiment Acc=0.7701, Val Topic Acc=0.8332\n",
      "Epoch 96/100, Loss=0.2133, Val Sentiment Acc=0.7764, Val Topic Acc=0.8272\n",
      "Epoch 97/100, Loss=0.2141, Val Sentiment Acc=0.7682, Val Topic Acc=0.8253\n",
      "Epoch 98/100, Loss=0.2098, Val Sentiment Acc=0.7780, Val Topic Acc=0.8310\n",
      "Epoch 99/100, Loss=0.2133, Val Sentiment Acc=0.7751, Val Topic Acc=0.8250\n",
      "Epoch 100/100, Loss=0.2138, Val Sentiment Acc=0.7770, Val Topic Acc=0.8307\n",
      "Model saved to: saved_model/vsfc_model.pth\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    num_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd1c212",
   "metadata": {},
   "source": [
    "**7. Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d0aa19f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_10840\\4183774725.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "saved_model = load_model(\n",
    "    model_class = VSFCClassifier,\n",
    "    vocab_size=vocab.get_vocab_size(),\n",
    "    embed_dim=128,\n",
    "    hidden_dim=256,\n",
    "    sentiment_classes=len(sentiment_labels),\n",
    "    topic_classes=len(topic_labels),\n",
    "    path='saved_model/vsfc_model.pth',\n",
    "    device=device,\n",
    "    pad_idx=vocab.word2idx[\"<PAD>\"]\n",
    "    )\n",
    "\n",
    "def test(model, test_loader, device, save_plot_dir=\"plots\"):\n",
    "    model.eval()\n",
    "    \n",
    "    correct_sent, total_sent = 0, 0\n",
    "    correct_topic, total_topic = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            sentiment_labels = batch[\"sentiment\"].to(device)\n",
    "            topic_labels = batch[\"topic\"].to(device)\n",
    "\n",
    "            s_logits, t_logits = model(input_ids)\n",
    "\n",
    "            s_pred = s_logits.argmax(dim=1)\n",
    "            t_pred = t_logits.argmax(dim=1)\n",
    "\n",
    "            correct_sent += (s_pred == sentiment_labels).sum().item()\n",
    "            total_sent += sentiment_labels.size(0)\n",
    "\n",
    "            correct_topic += (t_pred == topic_labels).sum().item()\n",
    "            total_topic += topic_labels.size(0)\n",
    "\n",
    "    sent_acc = correct_sent / total_sent\n",
    "    topic_acc = correct_topic / total_topic\n",
    "\n",
    "    print(f\"\\n========== TEST RESULT ==========\")\n",
    "    print(f\"Sentiment Accuracy: {sent_acc:.4f}\")\n",
    "    print(f\"Topic Accuracy:     {topic_acc:.4f}\")\n",
    "    print(\"=================================\\n\")\n",
    "\n",
    "    os.makedirs(save_plot_dir, exist_ok=True)\n",
    "\n",
    "    categories = [\"Sentiment\", \"Topic\"]\n",
    "    accuracies = [sent_acc, topic_acc]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(categories, accuracies)\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.title(\"Test Accuracy\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "\n",
    "    filename = os.path.join(save_plot_dir, \"test_accuracy.png\")\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "    return sent_acc, topic_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fb9b41b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== TEST RESULT ==========\n",
      "Sentiment Accuracy: 0.7896\n",
      "Topic Accuracy:     0.8345\n",
      "=================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7896399241945673, 0.8344914718888187)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(\n",
    "    model = saved_model,\n",
    "    test_loader=test_loader, \n",
    "    device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
